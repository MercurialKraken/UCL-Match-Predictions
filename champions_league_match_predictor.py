# -*- coding: utf-8 -*-
"""Champions League Match Predictor.ipynb

Automatically generated by Colaboratory.

# Data (Pre)-Processing
"""

#Importing libraries for data processing
import pandas as pd
import numpy as np

#Getting the data (fixtures, team stats, and stats against the team)
fixtures = pd.read_excel('Fixtures.xlsx').values
stats = pd.read_excel("Team Stats.xlsx").values
vs_stats = pd.read_excel("Vs Stats.xlsx").values

#Temp is used & re-used to visualize the intermediate steps
temp=[]
for i in fixtures:
    temp.append(str(i[1]).split("â€“"))
    
fixtures = np.delete(fixtures, 1, 1)
fixtures = np.append(fixtures, temp, 1)

fixturesArranged=[]
y = []
for i in fixtures:
    fixturesArranged.append([i[0], i[1]])
    y.append(i[2])           
    fixturesArranged.append([i[1], i[0]])
    y.append(i[3])

#Creating a function that would prepare our dataset for the neural network as a tensor
def process_fixtures(fixturesArranged):
    temp=[]
    temp2=[]
    for i in fixturesArranged:
        _t1 = str(i[0]).split()
        print(_t1)
        _t2 = str(i[1]).split()
        print(_t2)
        for j in stats:
            if ((_t1[0] in str(j[0])) and (_t1[1] in str(j[0]))):
                temp.append(j)
        for j in vs_stats:
            if ((_t2[0] in str(j[0])) and (_t2[1] in str(j[0]))):
                temp2.append(j)
                
    X = np.append(fixturesArranged, temp, 1)
    X = np.append(X, temp2, 1)
    X = np.delete(X, [0,1,2], 1)
    X = np.delete(X, 21, 1)
    return X

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
def split_data():
  X = process_fixtures(fixturesArranged)

  #Splitting data into training and testing datasets
  
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

  #Scaling the data appropriately to impart uniformity to the data

  X_train = sc.fit_transform(X_train)
  X_test = sc.transform(X_test)
  return X_train, X_test, y_train, y_test

"""# The Model"""

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
import os

checkpoint_path = "/content/fpl.ckpt" #@param{type:"string"}
checkpoint_dir = os.path.dirname(checkpoint_path)

import tensorflow as tf
es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=16)
ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_dir, monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')

def create_regressor():
  # Initialising the regressor
  regressor = Sequential()

  # input layer and first hidden layer
  regressor.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = 46))
  regressor.add(Dropout(0.2))

  # second hidden layer
  regressor.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))
  regressor.add(Dropout(0.2))

  # third hidden layer
  regressor.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))
  regressor.add(Dropout(0.2))

  # fourth hidden layer
  regressor.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))
  regressor.add(Dropout(0.2))

  # output layer
  regressor.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'linear'))

  # compiling the model
  regressor.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])
  
  return regressor

#RUN ONLY IF TRAINING
regressor = create_regressor()
X_train, X_test, y_train, y_test = split_data()
regressor.fit(np.asarray(X_train).astype('float32'), np.asarray(y_train).astype('float32'), batch_size = 10, epochs = 10000, shuffle = True, callbacks=[es,ckpt])

regressor.summary()

"""# Testing Model"""

# Not needed to run this if running from the beginning in the same session
regressor = create_regressor()
latest = tf.train.latest_checkpoint(checkpoint_dir)
latest
regressor.load_weights(latest)

# Evaluating the model


preds = regressor.predict(X_test)

y_test = np.array([int(i) for i in y_test])
print(y_test)
print(preds[:,0])

diff = preds[:,0] - y_test
print(diff)

mean = np.mean(diff)
std = np.std(diff)



print("[error_mean: {:.2f}, sample_error_stddev: {:.2f}".format(mean, std))

"""# Predicting New Unplayed Match (City Vs Inter)"""

fixturePred = [["Manchester City eng", "it Inter"], ["it Inter", "Manchester City eng"]] #ostrich algorithm :)
X_pred = process_fixtures(fixturePred)
X_pred = sc.transform(X_pred)
y_pred = np.reshape(regressor.predict(X_pred), (1,-1))[0]

print(y_pred)

import math
from scipy.stats import norm
z_score = ((y_pred[0]-y_pred[1])-mean)/math.sqrt(2*std)
p_value = norm.cdf(z_score) #probability that City wins given that errors made by the regressor are independent
print(p_value)
